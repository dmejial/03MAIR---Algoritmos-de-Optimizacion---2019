{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AUTOENCODERS_BOOSTING.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO+YsqnmavO6Zd5Mg2CFqsB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmejial/03MAIR---Algoritmos-de-Optimizacion---2019/blob/master/AUTOENCODERS_BOOSTING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAf7297i9kdl",
        "colab_type": "text"
      },
      "source": [
        "##CONFIG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQZrACmx8AcG",
        "colab_type": "text"
      },
      "source": [
        "conf_sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yEfL8Lf8D9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# HEADER FILE #\n",
        "from consensus.policy import combine_weighted\n",
        "from torch.nn import MSELoss\n",
        "from layer_decays.decay import LinearDecay\n",
        "\n",
        "######################################################\n",
        "__author__ = 'Hamed Sarvari, Bardh Prenkaj, Giovanni Stilo'\n",
        "######################################################\n",
        "# when constructing an AE, it indicates whether\n",
        "# you should use an odd number of layers or not\n",
        "# BOTTLENECK == True -> odd \n",
        "# BOTTLENECK == False -> even\n",
        "BOTTLENECK = True\n",
        "# The number of training epochs for each \n",
        "# iteration\n",
        "EPOCHS = 50\n",
        "# indicates the maximum of encoder layers to train\n",
        "# the model\n",
        "MAX_ENC_LAYERS = 4\n",
        "# The number of folds for which we repeat the\n",
        "# experiments\n",
        "FOLDS = 30\n",
        "# The maximum number of iterations\n",
        "ITERATIONS = 20\n",
        "# The loss optimisation function to use\n",
        "loss = MSELoss()\n",
        "# Consensus policy to adopt when combining the \n",
        "# reconstruction errors of each data point\n",
        "consensus = combine_weighted\n",
        "# shrinker that calculates the neurons\n",
        "# of the layers in the autoencoder\n",
        "shrinker = LinearDecay(0.5)\n",
        "# stop training threshold\n",
        "STOP_TH = 1e-4\n",
        "######################################################\n",
        "# Common Optimiser Parameters\n",
        "# Learning rate\n",
        "LR = 1e-3\n",
        "WEIGHT_DECAY = 1e-5\n",
        "######################################################\n",
        "# Flag that determines the data normalisation\n",
        "NORM = True\n",
        "# The value at the beginning of the normalisation\n",
        "# interval\n",
        "B_NORM = 0.0\n",
        "# The value at the end of the normalisation interval\n",
        "E_NORM = 1.0\n",
        "# Data Path\n",
        "DPATH = '../resources/data/'\n",
        "# The list of datasets to read and train on\n",
        "DATASETS = ['Lympho.csv']\n",
        "######################################################\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLxYwjle9qWQ",
        "colab_type": "text"
      },
      "source": [
        "##CONSENSUS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_Gp6ici8OzG",
        "colab_type": "text"
      },
      "source": [
        " policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gg4k_0Vz8WXV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import powerlaw\n",
        "from collections import Counter\n",
        "from scipy.stats import entropy\n",
        "\n",
        "__author__ = 'Hamed Sarvari, Bardh Prenkaj, Giovanni Stilo'\n",
        "\n",
        "def combine_sum(matrix):\n",
        "    return np.sum(matrix, axis=0)\n",
        "\n",
        "def combine_last(matrix):\n",
        "    return np.sum(matrix[-1], axis=0)\n",
        "\n",
        "def combine_best(matrix):\n",
        "    # get the row-wise sum of rec errors\n",
        "    s = np.sum(matrix, axis=1)\n",
        "\n",
        "    # get the first index of the component with the \n",
        "    # lowest rec error\n",
        "    min_rec_i = np.where(s == np.amin(s))[0][0]\n",
        "    # return all the rec errors of the data points\n",
        "    # in the min_rec_i-th row of the original matrix\n",
        "    return matrix[min_rec_i]\n",
        "\n",
        "def combine_fromsecond(matrix):\n",
        "    return np.sum(matrix[1:], axis=0)\n",
        "\n",
        "def combine_weighted(matrix):\n",
        "    # do not consider the first iteration of BAE\n",
        "    num = np.sum(matrix[1:], axis=1)\n",
        "    weights = (1 / num) / np.sum(1 / num)\n",
        "\n",
        "    return np.sum(matrix[1:] * weights[:, np.newaxis],\\\n",
        "        axis=0)\n",
        "\n",
        "def combine_kl_divergence(matrix):\n",
        "    # discard the first component from the ensemble\n",
        "    matrix = matrix[1:]\n",
        "\n",
        "    weights = list()\n",
        "    # loop through all the reconstruction errors\n",
        "    for rec_errors in matrix:\n",
        "        # fit a power law of the rec errors\n",
        "        res = powerlaw.Fit(rec_errors,\\\n",
        "        xmin=rec_errors.min(), xmax=rec_errors.max(),\\\n",
        "            discrete=True)\n",
        "        # get the occurrence probability of each\n",
        "        # data point\n",
        "        _, prob = res.ccdf()\n",
        "        # generate our own distribution of data\n",
        "        y = np.array(list(Counter(rec_errors).values()))\n",
        "        # scale it to [0,1]\n",
        "        y = y/y.max()\n",
        "        # calculate the kl-divergence between\n",
        "        # our distribution and the power law\n",
        "        score = entropy(y, prob)\n",
        "        # append the kl-divergence to the\n",
        "        # list of weights\n",
        "        weights.append(score)\n",
        "\n",
        "    weights = np.array(weights)\n",
        "\n",
        "    return np.sum(matrix * weights[:, np.newaxis], axis=0)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7QzACrr9zVS",
        "colab_type": "text"
      },
      "source": [
        "##LAYER_DECAY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v21CtqML8fAv",
        "colab_type": "text"
      },
      "source": [
        " decay"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVlxe9XV8fKi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "__author__ = 'Hamed Sarvari, Bardh Prenkaj, Giovanni Stilo'\n",
        "\n",
        "\"\"\" \n",
        "    This class represents the decay ratio from one\n",
        "    layer to the next in the DeepSymAE.\n",
        "\"\"\"\n",
        "class DecayAB(ABC):\n",
        " \n",
        "    def __init__(self, val):\n",
        "        self.val = val\n",
        "        super().__init__()\n",
        "    \n",
        "    @abstractmethod\n",
        "    def shrink(self, l, n_d):\n",
        "        return self.val * l * n_d\n",
        "\n",
        "class LinearDecay(DecayAB):\n",
        "    \n",
        "    def shrink(self, l, n_d):\n",
        "        return math.floor(self.val * n_d)\n",
        "\n",
        "class ExpDecay(DecayAB):\n",
        "\n",
        "    def shrink(self, l, n_d):\n",
        "        return math.ceil(n_d * math.exp(-(self.val * l)))\n",
        "\n",
        "class ConstantDecay(DecayAB):\n",
        "\n",
        "    def shrink(self, l, n_d):\n",
        "        return self.val[l]\n",
        "\n",
        "class CompositeDecay():\n",
        "    def __init__(self, shrinkers, choices, size):\n",
        "        self.shrinkers = shrinkers\n",
        "        self.choices = choices + [0]\n",
        "        self.size = size\n",
        "\n",
        "    def shrink(self, l, n_d):\n",
        "        for i in range(len(self.choices)):\n",
        "            if n_d/self.size >= self.choices[i]:\n",
        "                return self.shrinkers[i].shrink(l, n_d)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xe0KNmPe_4OA",
        "colab_type": "text"
      },
      "source": [
        "##MODELS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkN7BFlA8iUJ",
        "colab_type": "text"
      },
      "source": [
        "fcae"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiwbkK_48id8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "__author__ = 'Hamed Sarvari, Bardh Prenkaj, Giovanni Stilo'\n",
        "\n",
        "class FullyConnectedAE(nn.Module):\n",
        "\n",
        "    def __init__(self, enc_layers, dim,\\\n",
        "        shrinker, bottleneck=True, _min=3,\n",
        "        device='cpu'):\n",
        "\n",
        "        super(FullyConnectedAE, self).__init__()\n",
        "\n",
        "        self.enc_layers = enc_layers\n",
        "        self.dim = dim\n",
        "        self.min = _min\n",
        "        self.bottleneck = bottleneck\n",
        "        self.shrinker = shrinker\n",
        "        self.device = device\n",
        "\n",
        "        self.build()\n",
        "\n",
        "    def build(self):\n",
        "        layers = []\n",
        "        sizes = [self.dim]\n",
        "        \n",
        "        #Encoder Layers\n",
        "        for l in range(self.enc_layers):\n",
        "            n_h = max([self.shrinker.shrink(l+1,\\\n",
        "                sizes[-1]), self.min])\n",
        "\n",
        "            layers.append(nn.Linear(sizes[-1], n_h).to(self.device) )\n",
        "            # add a sigmoid layer as the first \n",
        "            # activation function\n",
        "            if l == 0:\n",
        "                layers.append(nn.Sigmoid().to(self.device))\n",
        "            else: # the other activation layers are ReLU\n",
        "                layers.append(nn.ReLU(inplace=False).to(self.device))\n",
        "\n",
        "            sizes.append(n_h)\n",
        "\n",
        "            if n_h == 1 or self.enc_layers == -1:\n",
        "                break\n",
        "\n",
        "        #Decoder Layers\n",
        "        for l in range(self.enc_layers, 1, -1):\n",
        "            layers.append(nn.Linear(sizes[l], sizes[l-1]).to(self.device))\n",
        "            layers.append(nn.ReLU(inplace=False).to(self.device))\n",
        "        \n",
        "        layers.append(nn.Linear(sizes[1], sizes[0]).to(self.device))\n",
        "        layers.append(nn.Sigmoid().to(self.device))\n",
        "\n",
        "\n",
        "        # if this flag is set then we need an\n",
        "        # odd number of layers\n",
        "        # the last layers is going to be placed at\n",
        "        # the centre of the model. Hence, the\n",
        "        # bottleneck naming. Because an\n",
        "        # autoencoder is symmetric, the bottleneck's\n",
        "        # input and output size are equal to the last\n",
        "        # encoder layer's output size.\n",
        "        if self.bottleneck == True:\n",
        "            # copy the encoder layers\n",
        "            temp = copy.deepcopy(layers[0:self.enc_layers*2])\n",
        "            # add the bottleneck\n",
        "            temp.append(nn.Linear(sizes[-1], sizes[-1]).to(self.device))\n",
        "            temp.append(nn.ReLU(inplace=False).to(self.device))\n",
        "            # add the old layers of the decoder\n",
        "            temp += layers[self.enc_layers*2:]\n",
        "            layers = copy.deepcopy(temp)\n",
        "\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "        print(self.net)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "    \"\"\"\n",
        "        Function that resets the parameter weights of \n",
        "        the layers in a neural network. The layers\n",
        "        in our case are all fully connected (torch.nn.Linear)\n",
        "\n",
        "        Params:\n",
        "            m (torch.nn.Module): the layer to reset weights\n",
        "    \"\"\"\n",
        "    def weight_reset(self, m):\n",
        "        if isinstance(m, torch.nn.Linear):\n",
        "            m.reset_parameters()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUXb5KCJ_8rh",
        "colab_type": "text"
      },
      "source": [
        "##SAMPLING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvqMGZnm8wU_",
        "colab_type": "text"
      },
      "source": [
        "samplers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1Qnnv2u8wen",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "import numpy as np\n",
        "\n",
        "__author__ = 'Hamed Sarvari, Bardh Prenkaj, Giovanni Stilo'\n",
        "\n",
        "\"\"\"\n",
        "    Class that performs the weighted sampling\n",
        "    based on the errors generated by the ensemble\n",
        "    components of BAE\n",
        "\"\"\"\n",
        "class WeightedSampler:\n",
        "\n",
        "    \n",
        "    def __init__(self, X):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            - X (torch.tensor): The original \n",
        "                    input tensor\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "\n",
        "    \"\"\"\n",
        "        This function takes a subset of the entire\n",
        "        tensor X and returns that subsample. The\n",
        "        subsample is selected with replacement.\n",
        "\n",
        "        Parameters:\n",
        "            - errors (numpy.array): The rec errors \n",
        "                    of iteration it\n",
        "            - it (int): The iteration of the training\n",
        "                    phase\n",
        "        return torch.tensor representing the subsample\n",
        "            of the original data\n",
        "    \"\"\"\n",
        "    def sample(self, errors, it):\n",
        "        # select all the instances in the first\n",
        "        # iterations\n",
        "        if it == 0:\n",
        "            return self.X\n",
        "        else:\n",
        "            # calculate the weights as described in\n",
        "            # the paper\n",
        "            weights = (1 / errors)/np.sum(1 / errors)\n",
        "            # sample the indices with replacement\n",
        "            indices = np.random.choice(list(range(\\\n",
        "                len(errors))), self.X.shape[0], p=weights)\n",
        "            # return the subsample\n",
        "            return self.X[indices]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RboS0BoGAAh5",
        "colab_type": "text"
      },
      "source": [
        "##STOPPAGE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zOgyUSC8z4W",
        "colab_type": "text"
      },
      "source": [
        "stoppers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3onHnV380AP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from numpy import linalg as la\n",
        "import torch\n",
        "\n",
        "__author__ = 'Hamed Sarvari, Bardh Prenkaj, Giovanni Stilo'\n",
        "\n",
        "class NormDiffStopper:\n",
        "    def __init__(self, precision, norm_kind=np.inf):\n",
        "        self.norm_kind = norm_kind\n",
        "        self.precision = precision\n",
        "\n",
        "    def stop(self, prev, curr, it):\n",
        "        dist = torch.abs(prev - curr)\n",
        "        return it >= 1 and la.norm(dist, self.norm_kind)/len(dist) <= self.precision\n",
        "\n",
        "class FixedStopper:\n",
        "    def __init__(self, maxit):\n",
        "        self.maxit = maxit\n",
        "\n",
        "\n",
        "    def stop(self, prev, curr, it):\n",
        "        return it  >= self.maxit\n",
        "\n",
        "\n",
        "class LossValueStopper:\n",
        "\n",
        "    def __init__(self, threshold):\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def stop(self, prev, curr, it):\n",
        "        return it > 1 and abs(prev - curr) < self.threshold\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqG8DACUAEEL",
        "colab_type": "text"
      },
      "source": [
        "##UTILS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB2Ibv0g9CvI",
        "colab_type": "text"
      },
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AU8QPhq9C-S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from scipy.io.arff import loadarff\n",
        "from scipy.io.matlab import loadmat\n",
        "from sklearn import preprocessing\n",
        "\n",
        "__author__ = 'Hamed Sarvari, Bardh Prenkaj, Giovanni Stilo'\n",
        "\n",
        "\n",
        "class DataLoader:\n",
        "\n",
        "    def __init__(self, filename, normalise=True,\\\n",
        "        b=-1.0, f=1.0):\n",
        "\n",
        "        self.filename = filename\n",
        "        self.normalise = normalise\n",
        "        self.b = b\n",
        "        self.f = f\n",
        "       \n",
        "\n",
        "    def __data_init(self):\n",
        "        try:\n",
        "            data = None\n",
        "\n",
        "            if self.filename.endswith('.arff'):\n",
        "                # load the arff file\n",
        "                data = loadarff(self.filename)\n",
        "\n",
        "                # transform the arff data into a pandas dataframe\n",
        "                data = pd.DataFrame(data[0])\n",
        "\n",
        "            elif self.filename.endswith('.csv'):\n",
        "                data = pd.read_csv(self.filename)\n",
        "\n",
        "            elif self.filename.endswith('.mat'):\n",
        "                data = loadmat(self.filename)\n",
        "\n",
        "                data = pd.DataFrame(np.hstack((data['X'],\\\n",
        "                    data['y'])))\n",
        "\n",
        "            # if the column 'id' is in the column list of the data,\n",
        "            # then remove it\n",
        "            if 'id' in data.columns:\n",
        "                data = data.drop(labels='id', axis=1)\n",
        "\n",
        "            dummies = pd.get_dummies(data[data.columns[-1]], drop_first=True)\n",
        "            data = data.drop(data.columns[-1], axis = 1)\n",
        "            data = data.join(dummies)\n",
        "\n",
        "            data = data.rename(columns={\\\n",
        "                data.columns[-1]: 'outlier'})\n",
        "\n",
        "            if self.normalise == True:\n",
        "                x = data.loc[:, data.columns != 'outlier'] \n",
        "                # scale column-wise data in [self.b, self.f]\n",
        "                # add the outlier column to the scaled data\n",
        "                data = self.__scale_data(x,\\\n",
        "                        self.b, self.f).join(data['outlier'])\n",
        "            \n",
        "            return data\n",
        "        except NotImplementedError:\n",
        "            print('The file %s contains not supported attributes.'\\\n",
        "                %self.filename)\n",
        "\n",
        "    def __scale_data(self, X, b, f, min=0, max=255, opt=True):\n",
        "        if opt == True:\n",
        "            max = X.max(axis=0)\n",
        "            min = X.min(axis=0)\n",
        "\n",
        "        diff = max - min\n",
        "\n",
        "        X = (f-b) * (X-min)/diff + b\n",
        "\n",
        "        # case in which the minimium and maximum of a column\n",
        "        # are the same\n",
        "        X.dropna(axis='columns')\n",
        "        X = X.fillna((f+b)/2)\n",
        "\n",
        "        return X\n",
        "\n",
        "\n",
        "    def prepare_data(self):\n",
        "        data = self.__data_init()\n",
        "        # get the ground truth labels\n",
        "        y = data['outlier'].values\n",
        "\n",
        "        if 'id' in data.columns:\n",
        "            X = data.drop(labels='id', axis=1)\n",
        "        else:\n",
        "            X = data\n",
        "        X = X.drop(labels='outlier', axis=1)\n",
        "\n",
        "        # normalise the data to [0,1]\n",
        "        X = torch.tensor(X.values, dtype=torch.float32)\n",
        "\n",
        "        print(\"Input tensor shape {0}\".format(X.shape))\n",
        "        print(\"Ground truth tensor shape {0}\".format(y.shape))\n",
        "\n",
        "\n",
        "        return X, y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ncEgYiu9J5y",
        "colab_type": "text"
      },
      "source": [
        "dump"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wk4ShW2B9KBD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gzip\n",
        "import json\n",
        "import os\n",
        "import uuid\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "__author__ = 'Hamed Sarvari, Bardh Prenkaj, Giovanni Stilo'\n",
        "\n",
        "NS_AN_SCORES = \"scores\"\n",
        "NS_REC_ERROR_IT = \"rec-error-it\"\n",
        "NS_RANKING =  \"ranking\"\n",
        "NS_WEIGHTS =  \"weights\"\n",
        "NS_LOSS = \"loss-value\"\n",
        "SEP =','\n",
        "\n",
        "class MetaInfo:\n",
        "\n",
        "    def __init__(self, params):\n",
        "\n",
        "        self.expid = uuid.uuid1().hex\n",
        "        self.params = params          \n",
        "        self.params['eid']=self.expid\n",
        "\n",
        "class ExperimentDumper:\n",
        "    \n",
        "    def __init__(self, meta_info, path=\"../resources/traces\"):\n",
        "\n",
        "        self.path=path\n",
        "        self.meta_info=meta_info\n",
        "        \n",
        "        self.namespaces = dict()\n",
        "\n",
        "        dirname = os.path.join(self.path,\\\n",
        "            self.meta_info.params['group'],\\\n",
        "                self.meta_info.expid)\n",
        "\n",
        "        self.meta_info.params['trace-path'] = dirname\n",
        "\n",
        "        os.makedirs(dirname)\n",
        "        \n",
        "        with open(os.path.join(self.path,\\\n",
        "            self.meta_info.expid \\\n",
        "                + '.json'), 'w') as outfile:  \n",
        "            json.dump(self.meta_info.params, outfile)\n",
        "\n",
        "    def trace_array(self,namespace,data):\n",
        "\n",
        "        self.get_writer(namespace)\\\n",
        "            .write(SEP.join([str(i) for i in data])\\\n",
        "                + '\\n')\n",
        "\n",
        "    def trace_model(self, model, it):\n",
        "        \n",
        "        torch.save(model, os.path.join(self.path,\\\n",
        "             self.meta_info.params['group'],\\\n",
        "                 self.meta_info.expid,\\\n",
        "                     model.__class__.__name__\\\n",
        "                         + \"@\" + it + \".pth\"))\n",
        "\n",
        "    def close(self):\n",
        "        for k in self.namespaces.keys():\n",
        "            self.namespaces[k].close()\n",
        "        \n",
        "    def get_writer(self, namespace):\n",
        "\n",
        "        if self.namespaces.get(namespace,None) is None:\n",
        "\n",
        "                self.namespaces[namespace] = gzip.open(\\\n",
        "                    os.path.join(self.path,\\\n",
        "                    self.meta_info.params['group'],\\\n",
        "                        self.meta_info.expid, namespace\\\n",
        "                            + \".gz\"), \"wt\")\n",
        "                self.namespaces[namespace].write('# '\\\n",
        "                + namespace + ' for :'\\\n",
        "                    + json.dumps(self.meta_info.params)\\\n",
        "                        + '\\n')\n",
        "        \n",
        "        return self.namespaces[namespace]\n",
        "        \n",
        "\n",
        "class LoadDump:\n",
        "\n",
        "    def __init__(self, path='../resources/traces'):\n",
        "\n",
        "        self.path = path\n",
        "        \n",
        "        self.descriptors = [f for f in \\\n",
        "            os.listdir(self.path) \\\n",
        "                if os.path.isfile(os.path\\\n",
        "                    .join(self.path, f))]\n",
        "\n",
        "\n",
        "    def filter(self, query):\n",
        "        results = []\n",
        "        for descriptor in self.descriptors:\n",
        "            data = json.load(open(os\\\n",
        "                .path.join(self.path,\\\n",
        "                    descriptor), 'r'))\n",
        "\n",
        "            if all(item in data.items()\\\n",
        "                for item in query.items()):\n",
        "\n",
        "                results.append(\\\n",
        "                    data.get('trace-path',None))\n",
        "\n",
        "        return results\n",
        "\n",
        "    def load(self, trace, doc=NS_REC_ERROR_IT):\n",
        "\n",
        "        try:\n",
        "            with gzip.open(os.path.join(trace,\\\n",
        "                doc + '.gz'), \"r\") as f:\n",
        "\n",
        "                res = pd.read_csv(f,\\\n",
        "                    skiprows=1, header=None).values\n",
        "\n",
        "                res = np.array(res, dtype=np.double)\n",
        "\n",
        "            return res\n",
        "        except:\n",
        "            return None\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wat0AuaEAK1m",
        "colab_type": "text"
      },
      "source": [
        "##MAIN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2GbR3SC9W3Q",
        "colab_type": "text"
      },
      "source": [
        "main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaclAl459WOl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import utils.dump as dumper\n",
        "from config import conf_sample as conf\n",
        "from models.fcae import FullyConnectedAE\n",
        "from sampling.samplers import WeightedSampler\n",
        "from utils.data import DataLoader\n",
        "from stoppage.stoppers import LossValueStopper\n",
        "\n",
        "__author__ = 'bardhp95'\n",
        "\n",
        "\n",
        "def get_model(enc_layers, dim):\n",
        "\n",
        "    model = FullyConnectedAE(enc_layers, dim,\\\n",
        "        conf.shrinker, bottleneck=conf.BOTTLENECK)\n",
        "\n",
        "    opt = torch.optim.Adam(model.parameters(),\\\n",
        "        lr=conf.LR,\\\n",
        "        amsgrad=True, weight_decay=conf.WEIGHT_DECAY)\n",
        "\n",
        "    return model, opt\n",
        "\n",
        "\"\"\"\n",
        "    Function that resets the reconstruction\n",
        "    errors to 0 after each fold is done\n",
        "    executing\n",
        "\n",
        "    Parameters:\n",
        "        - dim (int): represents the\n",
        "                dimensionality of the data\n",
        "                points\n",
        "\n",
        "    returns numpy array of zeros\n",
        "\"\"\"\n",
        "def reset_errors(dim):\n",
        "    return np.array([0 for i in range(dim)])\n",
        "\n",
        "\"\"\"\n",
        "    Function which gets the reconstruction errors\n",
        "    for each data point\n",
        "\n",
        "    Parameters:\n",
        "        - model (?): the trained torch model\n",
        "        - X (torch.tensor): the original data points\n",
        "    \n",
        "    returns numpy.array with the reconstruction errors\n",
        "        of each data point\n",
        "\"\"\"\n",
        "def get_error(model, X):\n",
        "    eval_err = torch.nn.MSELoss()\n",
        "    return np.array([eval_err(model(X[i]),\\\n",
        "        X[i]).data.item() for i in range(X.shape[0])])\n",
        "\n",
        "def shift_losses(losses, next_loss):\n",
        "    losses[0] = losses[-1]\n",
        "    losses[-1] = next_loss\n",
        "    return losses\n",
        "\n",
        "# define the training stopper\n",
        "stopper = LossValueStopper(conf.STOP_TH)\n",
        "# iterate the list of datasets\n",
        "for dataset in conf.DATASETS:\n",
        "    ############### Dataset loader ######################\n",
        "    loader = DataLoader(conf.DPATH + dataset,\\\n",
        "        normalise=conf.NORM, b=conf.B_NORM,\\\n",
        "            f=conf.E_NORM)\n",
        "\n",
        "    X, y = loader.prepare_data()\n",
        "\n",
        "    sampler = WeightedSampler(X)\n",
        "\n",
        "    for enc_layer in range(1,conf.MAX_ENC_LAYERS):\n",
        "        # what's the number of layers when taking in input\n",
        "        # the number of encoder layers? It depends if\n",
        "        # we're considering a single bottleneck or not\n",
        "        num_layers = enc_layer*2+1 if conf.BOTTLENECK else enc_layer*2\n",
        "        print(f'Training model with {num_layers} layers...')\n",
        "        # get the model with the specified number\n",
        "        # of encoder layers and the data\n",
        "        # dimensionality\n",
        "        model, opt = get_model(enc_layer, X.shape[1])\n",
        "        # perform experiments for the\n",
        "        # specified number of folds\n",
        "        for fold in range(conf.FOLDS):\n",
        "            rankings = list()\n",
        "            # reset the training data indices\n",
        "            rec_errors = reset_errors(X.shape[0])\n",
        "\n",
        "            print(f'Dataset {dataset} fold {fold+1}')\n",
        "            # Dump and loader characteristics\n",
        "            params = dict()\n",
        "            params['optimiser'] = opt.__class__.__name__\n",
        "            params['group'] = model.__class__.__name__\n",
        "            params['dataset'] = dataset\n",
        "            params['run'] = fold + 1\n",
        "            params['loss'] = conf.loss.__class__.__name__\n",
        "            params['normaliser'] = None\n",
        "            params['stopper'] = None\n",
        "            params['layer_num'] = num_layers\n",
        "\n",
        "            logger = dumper.ExperimentDumper(dumper\\\n",
        "                .MetaInfo(params))\n",
        "\n",
        "            # repeat the training for a certain amount \n",
        "            # of components\n",
        "            for it in range(conf.ITERATIONS):\n",
        "                # initialize the previous and current \n",
        "                # training losses\n",
        "                # losses[0] -> previous loss\n",
        "                # losses[1] -> current loss\n",
        "                losses = [-1,-1]\n",
        "                # reset the model's weights for the\n",
        "                # next AE component\n",
        "                model.apply(model.weight_reset)\n",
        "                # sample the virtual dataset \n",
        "                # \\mathbf{X}^{(i)}\n",
        "                # as described in the paper \n",
        "                X_i = sampler.sample(rec_errors, it)\n",
        "                # tell torch that the virtual dataset \n",
        "                # doesn't need to compute the gradients\n",
        "                X_i.requires_grad_(False)\n",
        "                \n",
        "                print(f'Training component {it+1}')\n",
        "                # train the components for a certain\n",
        "                # number of epochs\n",
        "                for epoch in range(conf.EPOCHS):\n",
        "                    model.train()\n",
        "                    # get the reonstructed output\n",
        "                    # of the virtual dataset \n",
        "                    # \\mathbf{X}^{(i)}\n",
        "                    X_i_out = model(X_i)\n",
        "                    # get the loss according to the \n",
        "                    # MSE criterion\n",
        "                    loss = conf.loss(X_i_out, X_i)\n",
        "\n",
        "                    losses = shift_losses(losses, loss)\n",
        "                    # zero the old gradients for the \n",
        "                    # backpropagation phase\n",
        "                    opt.zero_grad()\n",
        "                    # compute the backpropagation graph\n",
        "                    loss.backward()\n",
        "                    # update the weights of the ADAM \n",
        "                    # optimiser\n",
        "                    opt.step()\n",
        "\n",
        "                    if (epoch + 1) % 50 == 0:\n",
        "                        print(f'Epoch {epoch+1} - loss: {loss.item()}')\n",
        "\n",
        "                    if stopper.stop(losses[0], losses[1], epoch):\n",
        "                        print(f'Loss difference {abs(losses[0]-losses[1])} < {conf.STOP_TH}')\n",
        "                        print(f'Breaking the training of iteration {it+1} at epoch {epoch+1}')\n",
        "                        break\n",
        "                # evaluate the reconstruction errors of \n",
        "                # the trained\n",
        "                # model in the original datasetd\n",
        "                rec_errors = get_error(model, X)\n",
        "\n",
        "                print(f'Finished training component {it+1}')\n",
        "                print(f'Component {it+1}: loss {loss.item()} sum of rec_errors {np.sum(rec_errors)}')\n",
        "                # append the reconstruction errors in \n",
        "                # the ranking list\n",
        "                # this is used to compute the consensus \n",
        "                # function\n",
        "                rankings.append(rec_errors)\n",
        "                # trace reconstruction errors for each\n",
        "                # iteration\n",
        "                logger.trace_array(dumper\\\n",
        "                    .NS_REC_ERROR_IT, rec_errors)\n",
        "                # trace the loss of each iteration\n",
        "                logger.trace_array(dumper.NS_LOSS,\\\n",
        "                    [loss.data.numpy()])\n",
        "                # trace the model for each iteration\n",
        "                logger.trace_model(model, str(it + 1))\n",
        "\n",
        "            # combine the overall ranking\n",
        "            final_ranking = conf.consensus(rankings)\n",
        "            # log the overall ranking\n",
        "            logger.trace_array(dumper.NS_RANKING,\\\n",
        "                final_ranking)\n",
        "\n",
        "    print(f'Finished training BAE with a base architecture of {num_layers} layers.')\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}